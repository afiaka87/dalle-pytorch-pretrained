{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107ddda-e15f-439b-a831-12fe50cfe730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer_captions.json\")\n",
    "\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "def tokenize(texts, context_length = 256, add_start = False, add_end = False, truncate_text = False):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_tokens = tokenizer.encode(\"<|startoftext|>\").ids if add_start else []\n",
    "    eot_tokens = tokenizer.encode(\"<|endoftext|>\").ids if add_end else []\n",
    "    all_tokens = [sot_tokens + tokenizer.encode(text).ids + eot_tokens for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate_text:\n",
    "                tokens = tokens[:context_length]\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result\n",
    "\n",
    "!wget --no-clobber <dropbox_url>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0d9d2-b0a4-47e4-be51-8963aaa375bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-clobber https://www.dropbox.com/s/hl5hyzhyal3vfye/dalle_iconic_butterfly_149.pt\n",
    "%pip install tokenizers\n",
    "\n",
    "%pip install gpustat\n",
    "!git clone https://github.com/lucidrains/DALLE-pytorch\n",
    "%cd ./DALLE-pytorch/\n",
    "!python3 setup.py install\n",
    "!sudo apt-get -y install llvm-9-dev cmake\n",
    "!git clone https://github.com/microsoft/DeepSpeed.git /tmp/Deepspeed\n",
    "%cd /tmp/Deepspeed\n",
    "!DS_BUILD_SPARSE_ATTN=1 ./install.sh -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c5804-5f97-469f-8cf1-1cc8356591b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"dalle_iconic_butterfly_149.pt\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "text = \"an armchair imitating a pikachu. an armchair in the shape of a pikachu.\" #@param\n",
    "!python /content/DALLE-pytorch/generate.py --batch_size=32 --taming --dalle_path=$checkpoint_path --num_images=128 --text=\"$text\"; wait;\n",
    "text_cleaned = text.replace(\" \", \"_\")\n",
    "_folder = f\"/content/outputs/{text_cleaned}/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aea99b-c91d-40e9-97cb-d23b462fbc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "text_cleaned = text.replace(\" \", \"_\")\n",
    "output_dir = f\"/content/outputs/{text_cleaned}/\" #@param\n",
    "images = []\n",
    "for img_path in glob.glob(f'{output_dir}*.jpg'):\n",
    "    images.append(mpimg.imread(img_path))\n",
    "\n",
    "plt.figure(figsize=(128,128))\n",
    "columns = 5\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d91bf-ec7d-43d7-ba3c-4305ebd4cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"git+https://github.com/openai/CLIP.git\"\n",
    "import clip\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5300e3-d950-420f-add8-23aa5e005610",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get rank by CLIP! \"\"\"\n",
    "image = F.interpolate(images, size=224)\n",
    "text = clip.tokenize([\"this colorful bird has a yellow breast , with a black crown and a black cheek patch.\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d809f-32af-42f8-ad7a-698792e501ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_images = images.cpu().numpy()\n",
    "scores = probs[0]\n",
    "\n",
    "def show_reranking(images, scores, sort=True):\n",
    "    img_shape = images.shape\n",
    "    if sort:\n",
    "        scores_sort = scores.argsort()\n",
    "        scores = scores[scores_sort[::-1]]\n",
    "        images = images[scores_sort[::-1]]\n",
    "\n",
    "    rows = 4\n",
    "    cols = img_shape[0] // 4\n",
    "    img_idx = 0\n",
    "\n",
    "    for col in range(cols):\n",
    "        fig, axs = plt.subplots(1, rows, figsize=(20,20))\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "        for row in range(rows):\n",
    "            tran_img = np.transpose(images[img_idx], (1,2,0))\n",
    "            axs[row].imshow(tran_img, interpolation='nearest')\n",
    "            axs[row].set_title(\"{}%\".format(np.around(scores[img_idx]*100, 5)))\n",
    "            axs[row].set_xticks([])\n",
    "            axs[row].set_yticks([])\n",
    "            img_idx += 1\n",
    "\n",
    "show_reranking(np_images, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628e653-644a-418a-b554-203bea41d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "txt = \"this bird has wings that are brown with a white belly\"\n",
    "img_path = \"images/Yellow_Headed_Blackbird_0013_8362.jpg\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "tf = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "    transforms.RandomResizedCrop(256, scale=(0.6, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img = tf(img).cuda()\n",
    "\n",
    "sot_token = vocab.encode(\"<|startoftext|>\").ids[0]\n",
    "eot_token = vocab.encode(\"<|endoftext|>\").ids[0]\n",
    "codes = [0] * dalle_dict['hparams']['text_seq_len']\n",
    "text_token = vocab.encode(txt).ids\n",
    "tokens = [sot_token] + text_token + [eot_token]\n",
    "codes[:len(tokens)] = tokens\n",
    "caption_token = torch.LongTensor(codes).cuda()\n",
    "\n",
    "imgs = img.repeat(16,1,1,1)\n",
    "caps = caption_token.repeat(16,1)\n",
    "\n",
    "mask = (caps != 0).cuda()\n",
    "\n",
    "images = dalle.generate_images(\n",
    "        caps,\n",
    "        mask = mask,\n",
    "        img = imgs,\n",
    "        num_init_img_tokens = (100),  # you can set the size of the initial crop, defaults to a little less than ~1/2 of the tokens, as done in the paper\n",
    "        filter_thres = 0.9,\n",
    "        temperature = 1.0\n",
    ")\n",
    "\n",
    "grid = make_grid(images, nrow=4, normalize=False, range=(-1, 1)).cpu()\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f069a0-d859-4b5a-b100-079fdd609a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('afiaka87/dalle_train_transformer/trained-dalle:v14', type='model')\n",
    "# artifact_dir = artifact.download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
